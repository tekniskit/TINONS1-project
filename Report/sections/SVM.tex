%!TEX root = ..\Master.tex

\subsection{Support Vector Machines}
The support vector machine is a supervised learning algorithm. It can also be referred to as a sparse kernel machine. 
Compared to neural network, it sometimes gives a cleaner and more powerful way of learning complex non-linear functions. 
The goal of SVM is to maximise the margin between two clusters.

\subsubsection{Linear}
When doing the linear classifier, SVM will try to find the line that separates the data with the highest margin as seen on figure \ref{fig:svm-margin}.

\begin{figure}[H]
\centering
\includegraphics[scale=.75]{billeder/svm-margin}
\caption{Large margin classification.}
\label{fig:svm-margin}
\end{figure}

By solving the optimization problem defined as equation \ref{eq:svm-linear-optimization}, we find the parameters for the linear SVM classifier.
\begin{equation}
\min_{\theta}C \sum_{i=1}^{m}
\left[ y^{(i)}cost_1(\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\theta^Tx^{(i)}) \right]
+ \frac{1}{2}\sum_{i=1}^{n}\theta^2_j
\label{eq:svm-linear-optimization}
\end{equation}
We must choose the value of the constant $C$. \fxnote{(TODO: what is its effect? Some regularization thing. I think it is: C determines the weight of the slack variables)}
Figure \ref{fig:svm-cost-function} shows the plot of $cost_0$ and $cost_1$.
\begin{figure}[H]
\centering
\includegraphics[scale=.75]{billeder/svm-cost-function}
\caption{Left: $cost_1(\theta^Tx^{(i)})$. Right: $cost_0(\theta^Tx^{(i)})$}
\label{fig:svm-cost-function}
\end{figure}

Hypothesis: \fxnote{(TODO: Is this correct? shouldn't it be 1 if} $\theta^Tx \geq 1$ and 0 if $\theta^Tx \leq -1$)
\begin{equation}
h = 
\begin{cases}
1 & \text{if}\ \theta^Tx \geq 0 \\
0 & \text{otherwise}
\end{cases}
\end{equation}


\subsubsection{Non-linear}
When working with non-linear support vector machine the goal become to find the kernels that separates the data best. These kernels are transformations from one space to another. Linear kernels are define as $k(x_n,x_m)= x_n^Tx_m$ and Gaussian kernels can be defined as:
\begin{equation}
k(x_n,x_m) = e^{-\gamma|\bar{x}_n - \bar{x}_m|^2}
\end{equation}
\begin{figure}[H]
\centering
\includegraphics[scale=.75]{billeder/svm-non-linear}
\caption{Example of problem that requires non-linear classification.}
\label{fig:svm-non-linear}
\end{figure}

The gaussian kernel function:
\begin{equation}
f_i = similarity(x,l^{(i)}) =
exp\left(-\frac{||x-l^{(i)}||^2}{2\sigma^2}\right)
\end{equation}

At each training example we will put a landmark:
\begin{equation}
\begin{split}
\text{Given}\ (x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),\dots,(x^{(m)},y^{(m)}),\\
\text{choose}\ l^{(1)} = x^{(1)},l^{(2)} = x^{(2)},\dots,l^{(m)} = x^{(m)}
\end{split}
\end{equation}

We convert the training data into new feature vectors. For training example $(x^{(i)},y^{(i)})$:
\begin{equation}
f^{(i)}=
\begin{bmatrix}
f_0^{(i)} \\
f_1^{(i)} \\
\vdots \\
f_m^{(i)}
\end{bmatrix}
\end{equation}
 where $f_0^{(i)} = 1$.

The minimization problem now looks like:
\begin{equation}
\min_{\theta}C \sum_{i=1}^{m}
\left[ y^{(i)}cost_1(\theta^Tf^{(i)})+(1-y^{(i)})cost_0(\theta^Tf^{(i)}) \right]
+ \frac{1}{2}\sum_{i=1}^{m}\theta^2_j
\end{equation}

A large C gives lower bias and high variance, which means it uses a smaller amount of regularization and is more prone to overfitting. \\
A small C gives higher bias and low variance, which means it uses a higher amount of regularization and is more prone to underfitting. \\
A large $\sigma^2$ gives higher bias and lower variance, which means that features $f_i$ varies more smoothly. \\
A small $\sigma^2$ gives lower bias and higher variance, which means that features $f_i$ varies less smoothly.

In this project the softmax SVM is used with the constraint being:
\begin{equation}
w^* = ARG MIN (||w||^* + C \sum \epsilon_n)
\end{equation}
Inputting the features from this project along with a vector \textbf{t} that describes which class the features belong to results in the estimated weights for the SVM classifier. The support vector machine classifier is then used on the speaker recognition test data. When evaluating the test set a 21.68\% error was found. The confusion matrix can be seen in figure \ref{fig:conmatsvm}. 

\begin{figure}[H]
\centering
\includegraphics[scale=.75]{billeder/conmatsvm}
\caption{Confusion matrix for the support vector machine }
\label{fig:conmatsvm}
\end{figure}


%------------------------------------------------