\section{Discussion}
When looking at the results from table \ref{tab:restest}, it is easy to see that the linear methods: Linear classifier and support vector machines, is worse than the probabilistic methods. This is often the case if the classes is not clusterable, which make it hard to draw a good decision bound. The probabilistic methods is quite good in comparison, and the best match is archived by using the artificial neural network method, which only have a 12.41\% error. \\\ \\

Looking at how well the methods was able to classify the training data, as shown in figure \ref{tab:restrain}. It is seen that the support vector machine and the artificial neural network is capable of classifying the data perfectly whit no error. This is often a indication of over fitting, and you must therefore be very cautious whit the results. By making a extensive search in the solution set of these two methods, we found that this was also the best classifier of the test data, which tells us that over fitting was not the case. Instead this is a result of a very small test set, that makes it possible to completely satisfy the training scenario. This makes the classifiers very specialized. \\\ \\


When looking how the different classifiers perform on the different classes, it is shown that Rune is the one that is generally hardest to recognize. Only in the supervised Gaussian mixture model is this different where Rasmus is the hardest.  It is also seen that for the linear and Discriminative classifier, Rasmus is the easiest to find, but for the other models it is Nicolai. \\\ \\

All the presented methods in table \ref{tab:restest} is able to do classification of a speaker, since there are way more correct guesses than wrong. This can also be seen in figure \ref{fig:talktest}. It is also easy to see from this figure that support vector machines only struggle to classify Rune, but have no problem whit Nicolai and Rasmus. Figure \ref{tab:restest} also shows that the errors for the neural network is not happening in bursts, and is therefore filtered out \\\ \\

Looking at the PCA reduction in figure \ref{fig:DimError} it is seen how the error rate seems to stabilize at 30 features, regardless of algorithm used.


\section{Conclusion}
Using the a MFCC transformation of the audio data, we were able to get some representative features fore who is speaking. The classification was also used on features only extracted from the frequency using a FFT. But the MFCC features were way superior to the FFT features, so in order to get the simplest model, the FFT features were removed. \\\ \\

In order to further simplify the model, dimensionality reduction was used. Two methods of dimensionality reduction was tried, PCA and the Fisher method. Even though Fisher theoretically gives the best separation between the classes, it does have the limitation of only reducing to k-1 features. In this project there is only three classes, this means we max can get two features. This turns out to be too few features for good classification of the data. Using the PCA reduction, it is found that more than 30 PCA reduced features gives the best classification.  \\\ \\ 

A range of linear, probabilistic and sequential models have been used in the project to create a speaker recognition system. It is found that the lag of sequential dependency in the features makes the sequential model not suitable for this kind of application. The linear and probabilistic models all yield good results. The worst classifier found is the linear classifier that 22.22\% error. Even though this is the one that performs the worst, it is also the most simple to train and use. The best classifier found was the artificial neural network with an error rate of 12.41\%. Compared to the linear classifier is this a very hard to train, but is relative easy to use, when trained. \\\ \\  

When looking at the test result it is clear that the fairly small dataset have had an impact on the performance and for training a similar application for commercial use way more data is needed. 
