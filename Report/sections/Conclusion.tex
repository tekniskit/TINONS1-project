\section{Discussion}
When looking at the results from table \ref{tab:restest}, it is easy to see that the linear methods: Linear classifier and support vector machines, is worse than the probabilistic methods. This is often the case if the classes is not clusterable, which make it hard to draw a good decision bound. The probabilistic methods is quite good in comparison, and the best match is archived by using the artificial neural network method, which only have a 12.41\% error. \\\ \\

Looking at how well the methods was able to classify the training data, as shown in figure \ref{tab:restrain}. It is seen that the support vector machine and the artificial neural network is capable of classifying the data perfectly whit no error. This is often a indication of over fitting, and you must therefore be very cautious whit the results. By making a extensive search in the solution set of these two methods, we found that this was also the best classifier of the test data, which tells us that over fitting was not the case. Instead this is a result of a very small test set, that makes it possible to completely satisfy the training scenario. This makes the classifiers very specialized. \\\ \\


When looking how the different classifiers perform on the different classes, it is shown that Rune is the one that is generally hardest to recognize. Only in the supervised Gaussian mixture model is this different where Rasmus is the hardest.  It is also seen that for the linear and Discriminative classifier, Rasmus is the easiest to find, but for the other models it is Nicolai. \\\ \\

All the presented methods in table \ref{tab:restest} is able to do classification of a speaker, since there are way more correct guesses than wrong. This can also be seen in figure \ref{fig:talktest}. It is also easy to see from this figure that support vector machines only struggle to classify Rune, but have no problem whit Nicolai and Rasmus. Figure \ref{tab:restest} also shows that the errors for the neural network is not happening in bursts, and is therefore filtered out \\\ \\

Looking at the PCA reduction in figure \ref{fig:DimError} it is seen how the error rate seems to stabilize at 30 features, regardless of algorithm used.


\section{Conclusion}
Using the a MFCC transformation of the audio data, we ware able to get some representative features fore who is speaking. The classification was also tried on features only extracted from the frequency using a FFT. But the MFCC features was way superior to these, so in order to get the simplest model, the FFT features was removed. \\\ \\

In order to further simplify the model, was dimensionality reduction used. Two method of dimensionality reduction was tried PCA and the Fisher method. Even though Fisher theoretically gives the best separation between the classes, does it have the limitation of only reducing to k-1 features. Since we in this project only have three classes, this means we max can get two features. This turns out to be too few classes to do a good classification. Using the PCA reduction to is found that more than 30 PCA reduced features gives the best classification.  \\\ \\ 

In the project have a range of linear, probabilistic and sequential models been used to create a speaker recognition system. It is found that due to the lag of sequential dependency in the features, is the sequential model not suitable for this kind of application. Where the linear and probabilistic models all yields good results. The worst classifier found is the linear classifier that 22.22\% error. Event though this is the one that preforms the worst, it is also the most simple to train and use. The best classifier found was the artificial neural network which only have an error rate on 12.41\%. Compared to the linear classifier is this a very hard to train, but is relative easy to use, when trained. \\\ \\  

When looking at the test result it is clear that the fairly small dataset have had a impact on the performance, and for training a similar application for commercial use way more data is needed. 
